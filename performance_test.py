#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

Claude Code â‡„ Claude Desktop Bridgeã®æ€§èƒ½åŸºæº–ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import time
import os
from pathlib import Path
from automation_helper import (
    AutomationConfig,
    DesktopLauncher,
    ResponseMonitor,
    AutomatedBridge,
    ProposalExecutor,
    CheckpointManager
)

# psutilãŒãªã„å ´åˆã®ç°¡æ˜“å®Ÿè£…
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False


class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        if HAS_PSUTIL:
            self.process = psutil.Process(os.getpid())
        self.start_time = None
        self.start_memory = None
        self.start_cpu = None

    def start_measurement(self):
        """æ¸¬å®šé–‹å§‹"""
        self.start_time = time.time()
        if HAS_PSUTIL:
            self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            self.start_cpu = self.process.cpu_percent(interval=0.1)
        else:
            self.start_memory = 0
            self.start_cpu = 0

    def stop_measurement(self) -> dict:
        """æ¸¬å®šçµ‚äº†"""
        elapsed_time = time.time() - self.start_time

        if HAS_PSUTIL:
            end_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            memory_used = end_memory - self.start_memory
            cpu_percent = self.process.cpu_percent(interval=0.1)
        else:
            # psutilãŒãªã„å ´åˆã¯æ¨å®šå€¤
            end_memory = 0
            memory_used = 0
            cpu_percent = 0

        return {
            "elapsed_time": elapsed_time,
            "memory_used_mb": memory_used,
            "cpu_percent": cpu_percent,
            "total_memory_mb": end_memory
        }


def print_header(title: str):
    """ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")


def print_metrics(metrics: dict, requirements: dict):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨è¦ä»¶ã‚’è¡¨ç¤º"""
    print("æ¸¬å®šçµæœ:")
    print(f"  å®Ÿè¡Œæ™‚é–“: {metrics['elapsed_time']:.2f}ç§’")
    print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {metrics['memory_used_mb']:.2f}MB")
    print(f"  CPUä½¿ç”¨ç‡: {metrics['cpu_percent']:.1f}%")

    print("\nè¦ä»¶:")
    for key, value in requirements.items():
        print(f"  {key}: {value}")

    # åˆå¦åˆ¤å®š
    passed = True
    print("\nåˆ¤å®š:")

    if "max_time" in requirements:
        time_ok = metrics['elapsed_time'] <= requirements['max_time']
        print(f"  å®Ÿè¡Œæ™‚é–“: {'âœ… åˆæ ¼' if time_ok else 'âŒ ä¸åˆæ ¼'}")
        passed = passed and time_ok

    if "max_memory_mb" in requirements:
        memory_ok = metrics['memory_used_mb'] <= requirements['max_memory_mb']
        print(f"  ãƒ¡ãƒ¢ãƒª: {'âœ… åˆæ ¼' if memory_ok else 'âŒ ä¸åˆæ ¼'}")
        passed = passed and memory_ok

    if "max_cpu_percent" in requirements:
        cpu_ok = metrics['cpu_percent'] <= requirements['max_cpu_percent']
        print(f"  CPU: {'âœ… åˆæ ¼' if cpu_ok else 'âŒ ä¸åˆæ ¼'}")
        passed = passed and cpu_ok

    return passed


def test_requirement_6_1_launch_detection():
    """è¦ä»¶6.1: èµ·å‹•æ¤œçŸ¥ã¯5ç§’ä»¥å†…"""
    print_header("è¦ä»¶6.1: èµ·å‹•æ¤œçŸ¥æ€§èƒ½")

    config = AutomationConfig()
    config.auto_launch_desktop = False  # å®Ÿéš›ã®èµ·å‹•ã¯ã‚¹ã‚­ãƒƒãƒ—
    launcher = DesktopLauncher(config)

    metrics_obj = PerformanceMetrics()
    metrics_obj.start_measurement()

    # èµ·å‹•ãƒã‚§ãƒƒã‚¯ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆis_running ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
    for _ in range(5):
        launcher.is_running()
        time.sleep(0.1)

    metrics = metrics_obj.stop_measurement()

    requirements = {
        "max_time": 5.0,
        "èª¬æ˜": "èµ·å‹•æ¤œçŸ¥ã¯5ç§’ä»¥å†…ã«å®Œäº†ã™ã‚‹ã“ã¨"
    }

    return print_metrics(metrics, requirements)


def test_requirement_6_2_polling_efficiency():
    """è¦ä»¶6.2: ãƒãƒ¼ãƒªãƒ³ã‚°ã¯CPUä½¿ç”¨ç‡5%æœªæº€"""
    print_header("è¦ä»¶6.2: ãƒãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡")

    import tempfile

    config = AutomationConfig()
    temp_file = Path(tempfile.mktemp())

    try:
        monitor = ResponseMonitor(config, str(temp_file))

        metrics_obj = PerformanceMetrics()
        metrics_obj.start_measurement()

        # çŸ­æ™‚é–“ã®ãƒãƒ¼ãƒªãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        start_time = time.time()
        while time.time() - start_time < 2.0:
            monitor.check_for_response()
            time.sleep(config.polling_interval)

        metrics = metrics_obj.stop_measurement()

        requirements = {
            "max_cpu_percent": 5.0,
            "èª¬æ˜": "ãƒãƒ¼ãƒªãƒ³ã‚°ä¸­ã®CPUä½¿ç”¨ç‡ã¯5%æœªæº€ã§ã‚ã‚‹ã“ã¨"
        }

        return print_metrics(metrics, requirements)

    finally:
        if temp_file.exists():
            temp_file.unlink()


def test_requirement_6_3_workflow_speedup():
    """è¦ä»¶6.3: å®Œå…¨è‡ªå‹•åŒ–ã§50%æ™‚é–“çŸ­ç¸®"""
    print_header("è¦ä»¶6.3: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é«˜é€ŸåŒ–")

    config = AutomationConfig()
    config.auto_launch_desktop = False

    # æ‰‹å‹•ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ™‚é–“ã®è¦‹ç©ã‚‚ã‚Šï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
    manual_time = 60.0  # ç§’ï¼ˆæ‰‹å‹•æ“ä½œã‚’å«ã‚€æƒ³å®šæ™‚é–“ï¼‰

    # è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ™‚é–“ã®æ¸¬å®š
    metrics_obj = PerformanceMetrics()
    metrics_obj.start_measurement()

    bridge = AutomatedBridge(config)
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆã®ã¿ï¼ˆå®Ÿéš›ã®èµ·å‹•ã¨ç›£è¦–ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    request_id = bridge.create_automated_request(
        title="Performance Test",
        problem="Testing workflow speed",
        tried=["Manual process"],
        files_to_analyze=[]
    )

    metrics = metrics_obj.stop_measurement()

    # æ™‚é–“çŸ­ç¸®ç‡ã‚’è¨ˆç®—
    speedup_percent = ((manual_time - metrics['elapsed_time']) / manual_time) * 100

    print(f"æ‰‹å‹•ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¦‹ç©ã‚‚ã‚Š: {manual_time:.2f}ç§’")
    print(f"è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿæ¸¬: {metrics['elapsed_time']:.2f}ç§’")
    print(f"æ™‚é–“çŸ­ç¸®ç‡: {speedup_percent:.1f}%")

    requirements = {
        "ç›®æ¨™çŸ­ç¸®ç‡": "50%ä»¥ä¸Š",
        "èª¬æ˜": "è‡ªå‹•åŒ–ã«ã‚ˆã‚Š50%ä»¥ä¸Šã®æ™‚é–“çŸ­ç¸®ã‚’é”æˆã™ã‚‹ã“ã¨"
    }

    passed = speedup_percent >= 50.0
    print(f"\nåˆ¤å®š: {'âœ… åˆæ ¼' if passed else 'âŒ ä¸åˆæ ¼'}")

    return passed


def test_requirement_6_4_feedback_response():
    """è¦ä»¶6.4: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯2ç§’ä»¥å†…"""
    print_header("è¦ä»¶6.4: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¿œç­”æ€§")

    config = AutomationConfig()
    executor = ProposalExecutor(config)

    # ãƒ¢ãƒƒã‚¯ã‚¹ãƒ†ãƒƒãƒ—
    steps = [
        {"step": 1, "description": "Test step 1", "action": "Action 1"},
        {"step": 2, "description": "Test step 2", "action": "Action 2"}
    ]

    metrics_obj = PerformanceMetrics()
    metrics_obj.start_measurement()

    # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    for i, step in enumerate(steps, 1):
        executor.execute_step(step, i, len(steps))

    metrics = metrics_obj.stop_measurement()

    requirements = {
        "max_time": 2.0,
        "èª¬æ˜": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯2ç§’ä»¥å†…ã«è¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨"
    }

    return print_metrics(metrics, requirements)


def test_requirement_6_5_low_resource_impact():
    """è¦ä»¶6.5: ä½ãƒªã‚½ãƒ¼ã‚¹å½±éŸ¿"""
    print_header("è¦ä»¶6.5: ãƒªã‚½ãƒ¼ã‚¹å½±éŸ¿")

    import tempfile
    import shutil

    config = AutomationConfig()

    temp_dir = Path(tempfile.mkdtemp())
    test_files = []

    try:
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        for i in range(10):
            test_file = temp_dir / f"test_{i}.txt"
            test_file.write_text(f"Content {i}", encoding="utf-8")
            test_files.append(str(test_file))

        metrics_obj = PerformanceMetrics()
        metrics_obj.start_measurement()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        manager = CheckpointManager(config)
        checkpoint_id = manager.create_checkpoint(
            files=test_files,
            description="Performance test"
        )

        metrics = metrics_obj.stop_measurement()

        requirements = {
            "max_memory_mb": 50.0,
            "max_cpu_percent": 10.0,
            "èª¬æ˜": "ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã¯ãƒªã‚½ãƒ¼ã‚¹å½±éŸ¿ãŒæœ€å°é™ã§ã‚ã‚‹ã“ã¨"
        }

        return print_metrics(metrics, requirements)

    finally:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)


def run_all_performance_tests():
    """å…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print_header("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")

    tests = [
        ("è¦ä»¶6.1: èµ·å‹•æ¤œçŸ¥æ€§èƒ½", test_requirement_6_1_launch_detection),
        ("è¦ä»¶6.2: ãƒãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡", test_requirement_6_2_polling_efficiency),
        ("è¦ä»¶6.3: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é«˜é€ŸåŒ–", test_requirement_6_3_workflow_speedup),
        ("è¦ä»¶6.4: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¿œç­”æ€§", test_requirement_6_4_feedback_response),
        ("è¦ä»¶6.5: ãƒªã‚½ãƒ¼ã‚¹å½±éŸ¿", test_requirement_6_5_low_resource_impact)
    ]

    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆ '{name}' ã§ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # çµæœã‚µãƒãƒªãƒ¼
    print_header("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    for name, result in results:
        status = "âœ… åˆæ ¼" if result else "âŒ ä¸åˆæ ¼"
        print(f"  {name}: {status}")

    passed_count = sum(1 for _, result in results if result)
    total_count = len(results)

    print(f"\nç·åˆçµæœ: {passed_count}/{total_count} ãƒ†ã‚¹ãƒˆåˆæ ¼")

    if passed_count == total_count:
        print("\nğŸ‰ å…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãŒåˆæ ¼ã—ã¾ã—ãŸï¼")
        return 0
    else:
        print("\nâš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒä¸åˆæ ¼ã§ã™")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(run_all_performance_tests())
